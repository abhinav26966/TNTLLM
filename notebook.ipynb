{"cells":[{"source":"pip install langgraph langchain langchain_openai","metadata":{"executionCancelledAt":null,"executionTime":7456,"lastExecutedAt":1726830799972,"lastExecutedByKernel":"e58befa2-aa2a-4bd6-b8fa-faa904082c28","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"pip install langgraph langchain langchain_openai","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"cell_type":"code","id":"5a69afec-343c-4a78-a9c7-6c85cd6edf1e","outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nCollecting langgraph\n  Downloading langgraph-0.2.22-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.20)\nRequirement already satisfied: langchain_openai in /usr/local/lib/python3.10/dist-packages (0.1.7)\nCollecting langchain-core<0.4,>=0.2.39 (from langgraph)\n  Downloading langchain_core-0.3.2-py3-none-any.whl.metadata (6.3 kB)\nCollecting langgraph-checkpoint<2.0.0,>=1.0.2 (from langgraph)\n  Downloading langgraph_checkpoint-1.0.10-py3-none-any.whl.metadata (4.6 kB)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.7)\nRequirement already satisfied: langchain-community<0.1,>=0.0.38 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.38)\nINFO: pip is looking at multiple versions of langchain to determine which version is compatible with other requirements. This could take a while.\nCollecting langchain\n  Downloading langchain-0.3.0-py3-none-any.whl.metadata (7.1 kB)\nCollecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.79)\nRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\nCollecting pydantic<3.0.0,>=2.7.4 (from langchain)\n  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.3.0)\nINFO: pip is looking at multiple versions of langchain-openai to determine which version is compatible with other requirements. This could take a while.\nCollecting langchain_openai\n  Downloading langchain_openai-0.2.0-py3-none-any.whl.metadata (2.6 kB)\nCollecting openai<2.0.0,>=1.40.0 (from langchain_openai)\n  Downloading openai-1.46.1-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.7.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.2.39->langgraph) (1.33)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain)\n  Downloading langsmith-0.1.125-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.2.39->langgraph) (23.2)\nRequirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.2.39->langgraph) (4.11.0)\nCollecting msgpack<2.0.0,>=1.1.0 (from langgraph-checkpoint<2.0.0,>=1.0.2->langgraph)\n  Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.0)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.5)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (4.3.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.7.0)\nCollecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.40.0->langchain_openai)\n  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.3.1)\nRequirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (4.66.4)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\nCollecting pydantic-core==2.23.4 (from pydantic<3.0.0,>=2.7.4->langchain)\n  Downloading pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2023.12.25)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain_openai) (1.2.1)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.2.39->langgraph) (2.4)\nDownloading langgraph-0.2.22-py3-none-any.whl (98 kB)\nDownloading langchain-0.3.0-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_openai-0.2.0-py3-none-any.whl (51 kB)\nDownloading langchain_core-0.3.2-py3-none-any.whl (399 kB)\nDownloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\nDownloading langgraph_checkpoint-1.0.10-py3-none-any.whl (17 kB)\nDownloading langsmith-0.1.125-py3-none-any.whl (290 kB)\nDownloading openai-1.46.1-py3-none-any.whl (375 kB)\nDownloading pydantic-2.9.2-py3-none-any.whl (434 kB)\nDownloading pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\nDownloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\nInstalling collected packages: pydantic-core, msgpack, jiter, pydantic, openai, langsmith, langchain-core, langgraph-checkpoint, langchain-text-splitters, langchain_openai, langgraph, langchain\n\u001b[33m  WARNING: The script openai is installed in '/home/repl/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m  WARNING: The script langsmith is installed in '/home/repl/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m  WARNING: The script langchain-server is installed in '/home/repl/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncrewai 0.30.11 requires langchain<0.2.0,>=0.1.10, but you have langchain 0.3.0 which is incompatible.\nembedchain 0.1.110 requires langchain<0.2.0,>=0.1.4, but you have langchain 0.3.0 which is incompatible.\nembedchain 0.1.110 requires langchain-openai<0.2.0,>=0.1.7, but you have langchain-openai 0.2.0 which is incompatible.\nlangchain-cohere 0.1.5 requires langchain-core<0.3,>=0.1.42, but you have langchain-core 0.3.2 which is incompatible.\nlangchain-community 0.0.38 requires langchain-core<0.2.0,>=0.1.52, but you have langchain-core 0.3.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed jiter-0.5.0 langchain-0.3.0 langchain-core-0.3.2 langchain-text-splitters-0.3.0 langchain_openai-0.2.0 langgraph-0.2.22 langgraph-checkpoint-1.0.10 langsmith-0.1.125 msgpack-1.1.0 openai-1.46.1 pydantic-2.9.2 pydantic-core-2.23.4\nNote: you may need to restart the kernel to use updated packages.\n"}],"execution_count":1},{"source":"import os\n\nos.environ['AZURE_OPENAI_API_KEY'] = 'your_api_key_here'\nos.environ['AZURE_OPENAI_MODEL'] = 'your_deployment_name_here'\nos.environ['AZURE_OPENAI_ENDPOINT'] = 'deployment_endpoint'","metadata":{"executionCancelledAt":null,"executionTime":1570,"lastExecutedAt":1726830864535,"lastExecutedByKernel":"e58befa2-aa2a-4bd6-b8fa-faa904082c28","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import os\n\nos.environ['AZURE_OPENAI_API_KEY'] = 'your_api_key_here'\nos.environ['AZURE_OPENAI_MODEL'] = 'your_deployment_name_here'\nos.environ['AZURE_OPENAI_ENDPOINT'] = 'deployment_endpoint'"},"cell_type":"code","id":"8023069d-a6b1-495d-92d4-3012d301d6c2","outputs":[],"execution_count":3},{"source":"import operator\nfrom typing import Annotated, List, TypedDict, Optional\nclass Doc(TypedDict):\n    id: str\n    content: str\n    summary: Optional[str]\n    explanation: Optional[str]\n    category: Optional[str]\nclass TaxonomyGenerationState(TypedDict):\n    # The raw docs; we inject summaries within them in the first step\n    documents: List[Doc]\n    # Indices to be concise\n    minibatches: List[List[int]]\n    # Candidate Taxonomies (full trajectory)\n    clusters: Annotated[List[List[dict]], operator.add]","metadata":{"executionCancelledAt":null,"executionTime":10,"lastExecutedAt":1726830875272,"lastExecutedByKernel":"e58befa2-aa2a-4bd6-b8fa-faa904082c28","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import operator\nfrom typing import Annotated, List, TypedDict, Optional\nclass Doc(TypedDict):\n    id: str\n    content: str\n    summary: Optional[str]\n    explanation: Optional[str]\n    category: Optional[str]\nclass TaxonomyGenerationState(TypedDict):\n    # The raw docs; we inject summaries within them in the first step\n    documents: List[Doc]\n    # Indices to be concise\n    minibatches: List[List[int]]\n    # Candidate Taxonomies (full trajectory)\n    clusters: Annotated[List[List[dict]], operator.add]"},"cell_type":"code","id":"8a9794eb-fa38-4876-a6af-9dc645feb88b","outputs":[],"execution_count":5},{"source":"import pandas as pd\nfrom datasets import load_dataset\n# Load the dataset\ndataset = load_dataset(\"okite97/news-data\")\n# Access the different splits if available (e.g., train, test, validation)\ntrain_data = dataset['train']\ndf = pd.DataFrame(train_data)\ndf = df.dropna()\ndf.reset_index(drop=True, inplace=True)\ndef run_to_doc(df: pd.DataFrame) -> Doc:\n    all_data = []\n    for i in range(len(df)):\n        d = df.iloc[i]\n        all_data.append({\n            \"id\": i,\n            \"content\": d['Title'] + \"\\\\n\\\\n\" + d['Excerpt']\n        })\n    \n    return all_data\n# Only clustering 100 documents\ndocs = run_to_doc(df[:100])\nprint(docs[0])","metadata":{"executionCancelledAt":null,"executionTime":949,"lastExecutedAt":1726830899276,"lastExecutedByKernel":"e58befa2-aa2a-4bd6-b8fa-faa904082c28","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import pandas as pd\nfrom datasets import load_dataset\n# Load the dataset\ndataset = load_dataset(\"okite97/news-data\")\n# Access the different splits if available (e.g., train, test, validation)\ntrain_data = dataset['train']\ndf = pd.DataFrame(train_data)\ndf = df.dropna()\ndf.reset_index(drop=True, inplace=True)\ndef run_to_doc(df: pd.DataFrame) -> Doc:\n    all_data = []\n    for i in range(len(df)):\n        d = df.iloc[i]\n        all_data.append({\n            \"id\": i,\n            \"content\": d['Title'] + \"\\\\n\\\\n\" + d['Excerpt']\n        })\n    \n    return all_data\n# Only clustering 100 documents\ndocs = run_to_doc(df[:100])\nprint(docs[0])","outputsMetadata":{"0":{"height":80,"type":"stream"},"5":{"height":80,"type":"stream"}}},"cell_type":"code","id":"05e66628-a25b-4d73-ae43-a737c8b62f1d","outputs":[{"output_type":"stream","name":"stdout","text":"{'id': 0, 'content': 'Uefa Opens Proceedings against Barcelona, Juventus and Real Madrid Over European Super League Plan\\\\n\\\\nUefa has opened disciplinary proceedings against Barcelona, Juventus and Real Madrid over their involvement in the proposed European Super League.'}\n"}],"execution_count":8},{"source":"import os\nfrom langchain_openai import AzureChatOpenAI\n\nmodel = AzureChatOpenAI(\n    openai_api_version=\"2023-06-01-preview\",\n    azure_deployment=\"gpt-4o-2024-05-13\",\n    temperature=0.0\n)","metadata":{"executionCancelledAt":null,"executionTime":1326,"lastExecutedAt":1726831006428,"lastExecutedByKernel":"e58befa2-aa2a-4bd6-b8fa-faa904082c28","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import os\nfrom langchain_openai import AzureChatOpenAI\n\nmodel = AzureChatOpenAI(\n    openai_api_version=\"2023-06-01-preview\",\n    azure_deployment=\"gpt-4o-2024-05-13\",\n    temperature=0.0\n)"},"cell_type":"code","id":"b4b1485f-1d3a-41f2-89a1-848c8196fa09","outputs":[],"execution_count":12},{"source":"import re\nimport random\nfrom langchain import hub\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnableConfig, RunnableLambda, RunnablePassthrough\nsummary_prompt = hub.pull(\"wfh/tnt-llm-summary-generation\").partial(\n    summary_length=20, explanation_length=30\n)\ndef parse_summary(xml_string: str) -> dict:\n    summary_pattern = r\"<summary>(.*?)</summary>\"\n    explanation_pattern = r\"<explanation>(.*?)</explanation>\"\n    summary_match = re.search(summary_pattern, xml_string, re.DOTALL)\n    explanation_match = re.search(explanation_pattern, xml_string, re.DOTALL)\n    summary = summary_match.group(1).strip() if summary_match else \"\"\n    explanation = explanation_match.group(1).strip() if explanation_match else \"\"\n    return {\"summary\": summary, \"explanation\": explanation}\nsummary_llm_chain = (\n    summary_prompt | model | StrOutputParser()\n).with_config(run_name=\"GenerateSummary\")\nsummary_chain = summary_llm_chain | parse_summary\n# Input: state\n# Output: state and summaries\n# Processes docs in parallel\ndef get_content(state: TaxonomyGenerationState):\n    docs = state[\"documents\"]\n    return [{\"content\": doc[\"content\"]} for doc in docs]\nmap_step = RunnablePassthrough.assign(\n    summaries=get_content\n    | RunnableLambda(func=summary_chain.batch, afunc=summary_chain.abatch)\n)\ndef reduce_summaries(combined: dict) -> TaxonomyGenerationState:\n    summaries = combined[\"summaries\"]\n    documents = combined[\"documents\"]\n    return {\n        \"documents\": [\n            {\n                \"id\": doc[\"id\"],\n                \"content\": doc[\"content\"],\n                \"summary\": summ_info[\"summary\"],\n                \"explanation\": summ_info[\"explanation\"],\n            }\n            for doc, summ_info in zip(documents, summaries)\n        ]\n    }\n# This is the summary node\nmap_reduce_chain = map_step | reduce_summaries\nprint(map_reduce_chain)","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":101,"type":"stream"}}},"cell_type":"code","id":"3fb5a4f9-24d1-4ff6-bbcf-61719eed3a4b","outputs":[{"output_type":"stream","name":"stdout","text":"first=RunnableAssign(mapper={\n  summaries: RunnableLambda(get_content)\n             | RunnableLambda(batch)\n}) middle=[] last=RunnableLambda(reduce_summaries)\n"}],"execution_count":16},{"source":"def get_minibatches(state: TaxonomyGenerationState, config: RunnableConfig):\n    batch_size = config[\"configurable\"].get(\"batch_size\", 200)\n    original = state[\"documents\"]\n    indices = list(range(len(original)))\n    random.shuffle(indices)\n    if len(indices) < batch_size:\n        # Don't pad needlessly if we can't fill a single batch\n        return [indices]\n    num_full_batches = len(indices) // batch_size\n    batches = [\n        indices[i * batch_size : (i + 1) * batch_size] for i in range(num_full_batches)\n    ]\n    leftovers = len(indices) % batch_size\n    if leftovers:\n        last_batch = indices[num_full_batches * batch_size :]\n        elements_to_add = batch_size - leftovers\n        last_batch += random.sample(indices, elements_to_add)\n        batches.append(last_batch)\n    return {\n        \"minibatches\": batches,\n    }","metadata":{"executionCancelledAt":null,"executionTime":12,"lastExecutedAt":1726831056109,"lastExecutedByKernel":"e58befa2-aa2a-4bd6-b8fa-faa904082c28","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def get_minibatches(state: TaxonomyGenerationState, config: RunnableConfig):\n    batch_size = config[\"configurable\"].get(\"batch_size\", 200)\n    original = state[\"documents\"]\n    indices = list(range(len(original)))\n    random.shuffle(indices)\n    if len(indices) < batch_size:\n        # Don't pad needlessly if we can't fill a single batch\n        return [indices]\n    num_full_batches = len(indices) // batch_size\n    batches = [\n        indices[i * batch_size : (i + 1) * batch_size] for i in range(num_full_batches)\n    ]\n    leftovers = len(indices) % batch_size\n    if leftovers:\n        last_batch = indices[num_full_batches * batch_size :]\n        elements_to_add = batch_size - leftovers\n        last_batch += random.sample(indices, elements_to_add)\n        batches.append(last_batch)\n    return {\n        \"minibatches\": batches,\n    }"},"cell_type":"code","id":"130c07c7-117b-41b7-bebc-1fdece41b750","outputs":[],"execution_count":18},{"source":"from typing import Dict\nfrom langchain_core.runnables import Runnable\ndef parse_taxa(output_text: str) -> Dict:\n    \"\"\"Extract the taxonomy from the generated output.\"\"\"\n    cluster_pattern = r\"<cluster>\\\\s*<id>(.*?)</id>\\\\s*<name>(.*?)</name>\\\\s*<description>(.*?)</description>\\\\s*</cluster>\"\n    cluster_matches = re.findall(cluster_pattern, output_text, re.DOTALL)\n    clusters = [\n        {\"id\": id.strip(), \"name\": name.strip(), \"description\": description.strip()}\n        for id, name, description in cluster_matches\n    ]\n    return {\"clusters\": clusters}\ndef format_docs(docs: List[Doc]) -> str:\n    xml_table = \"\\\\n\"\n    for doc in docs:\n        xml_table += f'{doc[\"summary\"]}\\\\n'\n    xml_table += \"\"\n    return xml_table\ndef format_taxonomy(clusters):\n    xml = \"\\\\n\"\n    for label in clusters:\n        xml += \"  \\\\n\"\n        xml += f'    {label[\"id\"]}\\\\n'\n        xml += f'    {label[\"name\"]}\\\\n'\n        xml += f'    {label[\"description\"]}\\\\n'\n        xml += \"  \\\\n\"\n    xml += \"\"\n    return xml\ndef invoke_taxonomy_chain(\n    chain: Runnable,\n    state: TaxonomyGenerationState,\n    config: RunnableConfig,\n    mb_indices: List[int],\n) -> TaxonomyGenerationState:\n    configurable = config[\"configurable\"]\n    docs = state[\"documents\"]\n    minibatch = [docs[idx] for idx in mb_indices]\n    data_table_xml = format_docs(minibatch)\n    previous_taxonomy = state[\"clusters\"][-1] if state[\"clusters\"] else []\n    cluster_table_xml = format_taxonomy(previous_taxonomy)\n    updated_taxonomy = chain.invoke(\n        {\n            \"data_xml\": data_table_xml,\n            \"use_case\": configurable[\"use_case\"],\n            \"cluster_table_xml\": cluster_table_xml,\n            \"suggestion_length\": configurable.get(\"suggestion_length\", 30),\n            \"cluster_name_length\": configurable.get(\"cluster_name_length\", 10),\n            \"cluster_description_length\": configurable.get(\n                \"cluster_description_length\", 30\n            ),\n            \"explanation_length\": configurable.get(\"explanation_length\", 20),\n            \"max_num_clusters\": configurable.get(\"max_num_clusters\", 25),\n        }\n    )\n    return {\n        \"clusters\": [updated_taxonomy[\"clusters\"]],\n    }","metadata":{"executionCancelledAt":null,"executionTime":14,"lastExecutedAt":1726831072990,"lastExecutedByKernel":"e58befa2-aa2a-4bd6-b8fa-faa904082c28","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from typing import Dict\nfrom langchain_core.runnables import Runnable\ndef parse_taxa(output_text: str) -> Dict:\n    \"\"\"Extract the taxonomy from the generated output.\"\"\"\n    cluster_pattern = r\"<cluster>\\\\s*<id>(.*?)</id>\\\\s*<name>(.*?)</name>\\\\s*<description>(.*?)</description>\\\\s*</cluster>\"\n    cluster_matches = re.findall(cluster_pattern, output_text, re.DOTALL)\n    clusters = [\n        {\"id\": id.strip(), \"name\": name.strip(), \"description\": description.strip()}\n        for id, name, description in cluster_matches\n    ]\n    return {\"clusters\": clusters}\ndef format_docs(docs: List[Doc]) -> str:\n    xml_table = \"\\\\n\"\n    for doc in docs:\n        xml_table += f'{doc[\"summary\"]}\\\\n'\n    xml_table += \"\"\n    return xml_table\ndef format_taxonomy(clusters):\n    xml = \"\\\\n\"\n    for label in clusters:\n        xml += \"  \\\\n\"\n        xml += f'    {label[\"id\"]}\\\\n'\n        xml += f'    {label[\"name\"]}\\\\n'\n        xml += f'    {label[\"description\"]}\\\\n'\n        xml += \"  \\\\n\"\n    xml += \"\"\n    return xml\ndef invoke_taxonomy_chain(\n    chain: Runnable,\n    state: TaxonomyGenerationState,\n    config: RunnableConfig,\n    mb_indices: List[int],\n) -> TaxonomyGenerationState:\n    configurable = config[\"configurable\"]\n    docs = state[\"documents\"]\n    minibatch = [docs[idx] for idx in mb_indices]\n    data_table_xml = format_docs(minibatch)\n    previous_taxonomy = state[\"clusters\"][-1] if state[\"clusters\"] else []\n    cluster_table_xml = format_taxonomy(previous_taxonomy)\n    updated_taxonomy = chain.invoke(\n        {\n            \"data_xml\": data_table_xml,\n            \"use_case\": configurable[\"use_case\"],\n            \"cluster_table_xml\": cluster_table_xml,\n            \"suggestion_length\": configurable.get(\"suggestion_length\", 30),\n            \"cluster_name_length\": configurable.get(\"cluster_name_length\", 10),\n            \"cluster_description_length\": configurable.get(\n                \"cluster_description_length\", 30\n            ),\n            \"explanation_length\": configurable.get(\"explanation_length\", 20),\n            \"max_num_clusters\": configurable.get(\"max_num_clusters\", 25),\n        }\n    )\n    return {\n        \"clusters\": [updated_taxonomy[\"clusters\"]],\n    }"},"cell_type":"code","id":"40b7e9fc-ab79-4588-b70b-8525c57a1136","outputs":[],"execution_count":20},{"source":"# We will share an LLM for each step of the generate -> update -> review cycle\n# You may want to consider using Opus or another more powerful model for this\ntaxonomy_generation_llm = model\n## Initial generation\ntaxonomy_generation_prompt = hub.pull(\"wfh/tnt-llm-taxonomy-generation\").partial(\n    use_case=\"Generate the taxonomy that can be used to label the user intent in the conversation.\",\n)\ntaxa_gen_llm_chain = (\n    taxonomy_generation_prompt | taxonomy_generation_llm | StrOutputParser()\n).with_config(run_name=\"GenerateTaxonomy\")\ngenerate_taxonomy_chain = taxa_gen_llm_chain | parse_taxa\ndef generate_taxonomy(\n    state: TaxonomyGenerationState, config: RunnableConfig\n) -> TaxonomyGenerationState:\n    return invoke_taxonomy_chain(\n        generate_taxonomy_chain, state, config, state[\"minibatches\"][0]\n    )","metadata":{"executionCancelledAt":null,"executionTime":213,"lastExecutedAt":1726831085220,"lastExecutedByKernel":"e58befa2-aa2a-4bd6-b8fa-faa904082c28","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# We will share an LLM for each step of the generate -> update -> review cycle\n# You may want to consider using Opus or another more powerful model for this\ntaxonomy_generation_llm = model\n## Initial generation\ntaxonomy_generation_prompt = hub.pull(\"wfh/tnt-llm-taxonomy-generation\").partial(\n    use_case=\"Generate the taxonomy that can be used to label the user intent in the conversation.\",\n)\ntaxa_gen_llm_chain = (\n    taxonomy_generation_prompt | taxonomy_generation_llm | StrOutputParser()\n).with_config(run_name=\"GenerateTaxonomy\")\ngenerate_taxonomy_chain = taxa_gen_llm_chain | parse_taxa\ndef generate_taxonomy(\n    state: TaxonomyGenerationState, config: RunnableConfig\n) -> TaxonomyGenerationState:\n    return invoke_taxonomy_chain(\n        generate_taxonomy_chain, state, config, state[\"minibatches\"][0]\n    )"},"cell_type":"code","id":"aa6808dc-65f0-492f-89b5-53f80d4678b1","outputs":[],"execution_count":22},{"source":"taxonomy_update_prompt = hub.pull(\"wfh/tnt-llm-taxonomy-update\")\ntaxa_update_llm_chain = (\n    taxonomy_update_prompt | taxonomy_generation_llm | StrOutputParser()\n).with_config(run_name=\"UpdateTaxonomy\")\nupdate_taxonomy_chain = taxa_update_llm_chain | parse_taxa\ndef update_taxonomy(\n    state: TaxonomyGenerationState, config: RunnableConfig\n) -> TaxonomyGenerationState:\n    which_mb = len(state[\"clusters\"]) % len(state[\"minibatches\"])\n    return invoke_taxonomy_chain(\n        update_taxonomy_chain, state, config, state[\"minibatches\"][which_mb]\n    )","metadata":{"executionCancelledAt":null,"executionTime":217,"lastExecutedAt":1726831094008,"lastExecutedByKernel":"e58befa2-aa2a-4bd6-b8fa-faa904082c28","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"taxonomy_update_prompt = hub.pull(\"wfh/tnt-llm-taxonomy-update\")\ntaxa_update_llm_chain = (\n    taxonomy_update_prompt | taxonomy_generation_llm | StrOutputParser()\n).with_config(run_name=\"UpdateTaxonomy\")\nupdate_taxonomy_chain = taxa_update_llm_chain | parse_taxa\ndef update_taxonomy(\n    state: TaxonomyGenerationState, config: RunnableConfig\n) -> TaxonomyGenerationState:\n    which_mb = len(state[\"clusters\"]) % len(state[\"minibatches\"])\n    return invoke_taxonomy_chain(\n        update_taxonomy_chain, state, config, state[\"minibatches\"][which_mb]\n    )"},"cell_type":"code","id":"2766ebfa-5986-49b8-9dd3-e34cb2091580","outputs":[],"execution_count":23},{"source":"taxonomy_review_prompt = hub.pull(\"wfh/tnt-llm-taxonomy-review\")\ntaxa_review_llm_chain = (\n    taxonomy_review_prompt | taxonomy_generation_llm | StrOutputParser()\n).with_config(run_name=\"ReviewTaxonomy\")\nreview_taxonomy_chain = taxa_review_llm_chain | parse_taxa\ndef review_taxonomy(\n    state: TaxonomyGenerationState, config: RunnableConfig\n) -> TaxonomyGenerationState:\n    batch_size = config[\"configurable\"].get(\"batch_size\", 200)\n    original = state[\"documents\"]\n    indices = list(range(len(original)))\n    random.shuffle(indices)\n    return invoke_taxonomy_chain(\n        review_taxonomy_chain, state, config, indices[:batch_size]\n    )","metadata":{"executionCancelledAt":null,"executionTime":212,"lastExecutedAt":1726831100892,"lastExecutedByKernel":"e58befa2-aa2a-4bd6-b8fa-faa904082c28","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"taxonomy_review_prompt = hub.pull(\"wfh/tnt-llm-taxonomy-review\")\ntaxa_review_llm_chain = (\n    taxonomy_review_prompt | taxonomy_generation_llm | StrOutputParser()\n).with_config(run_name=\"ReviewTaxonomy\")\nreview_taxonomy_chain = taxa_review_llm_chain | parse_taxa\ndef review_taxonomy(\n    state: TaxonomyGenerationState, config: RunnableConfig\n) -> TaxonomyGenerationState:\n    batch_size = config[\"configurable\"].get(\"batch_size\", 200)\n    original = state[\"documents\"]\n    indices = list(range(len(original)))\n    random.shuffle(indices)\n    return invoke_taxonomy_chain(\n        review_taxonomy_chain, state, config, indices[:batch_size]\n    )"},"cell_type":"code","id":"08bff059-0fc7-48ff-9346-8818a5d27f5d","outputs":[],"execution_count":24},{"source":"from langgraph.graph import StateGraph\ngraph = StateGraph(TaxonomyGenerationState)\ngraph.add_node(\"summarize\", map_reduce_chain)\ngraph.add_node(\"get_minibatches\", get_minibatches)\ngraph.add_node(\"generate_taxonomy\", generate_taxonomy)\ngraph.add_node(\"update_taxonomy\", update_taxonomy)\ngraph.add_node(\"review_taxonomy\", review_taxonomy)\ngraph.add_edge(\"summarize\", \"get_minibatches\")\ngraph.add_edge(\"get_minibatches\", \"generate_taxonomy\")\ngraph.add_edge(\"generate_taxonomy\", \"update_taxonomy\")\ndef should_review(state: TaxonomyGenerationState) -> str:\n    num_minibatches = len(state[\"minibatches\"])\n    num_revisions = len(state[\"clusters\"])\n    if num_revisions < num_minibatches:\n        return \"update_taxonomy\"\n    return \"review_taxonomy\"\ngraph.add_conditional_edges(\n    \"update_taxonomy\",\n    should_review,\n    # Optional (but required for the diagram to be drawn correctly below)\n    {\"update_taxonomy\": \"update_taxonomy\", \"review_taxonomy\": \"review_taxonomy\"},\n)\ngraph.set_finish_point(\"review_taxonomy\")\ngraph.set_entry_point(\"summarize\")\napp = graph.compile()","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1726831108434,"lastExecutedByKernel":"e58befa2-aa2a-4bd6-b8fa-faa904082c28","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langgraph.graph import StateGraph\ngraph = StateGraph(TaxonomyGenerationState)\ngraph.add_node(\"summarize\", map_reduce_chain)\ngraph.add_node(\"get_minibatches\", get_minibatches)\ngraph.add_node(\"generate_taxonomy\", generate_taxonomy)\ngraph.add_node(\"update_taxonomy\", update_taxonomy)\ngraph.add_node(\"review_taxonomy\", review_taxonomy)\ngraph.add_edge(\"summarize\", \"get_minibatches\")\ngraph.add_edge(\"get_minibatches\", \"generate_taxonomy\")\ngraph.add_edge(\"generate_taxonomy\", \"update_taxonomy\")\ndef should_review(state: TaxonomyGenerationState) -> str:\n    num_minibatches = len(state[\"minibatches\"])\n    num_revisions = len(state[\"clusters\"])\n    if num_revisions < num_minibatches:\n        return \"update_taxonomy\"\n    return \"review_taxonomy\"\ngraph.add_conditional_edges(\n    \"update_taxonomy\",\n    should_review,\n    # Optional (but required for the diagram to be drawn correctly below)\n    {\"update_taxonomy\": \"update_taxonomy\", \"review_taxonomy\": \"review_taxonomy\"},\n)\ngraph.set_finish_point(\"review_taxonomy\")\ngraph.set_entry_point(\"summarize\")\napp = graph.compile()"},"cell_type":"code","id":"95ab06d4-30eb-4e23-ade7-833b8201951b","outputs":[],"execution_count":25},{"source":"use_case = (\n    \"Generate the taxonomy that can be used to label the news article that would benefit the user.\"\n)\ntry:\n    stream = app.stream(\n        {\"documents\": docs},\n        {\n            \"configurable\": {\n                \"use_case\": use_case,\n                # Optional:\n                \"batch_size\": 10,\n                \"suggestion_length\": 30,\n                \"cluster_name_length\": 10,\n                \"cluster_description_length\": 30,\n                \"explanation_length\": 20,\n                \"max_num_clusters\": 25,\n            },\n            \"max_concurrency\": 2,\n            \"recursion_limit\": 50,\n        },\n    )\n    for step in stream:\n        node, state = next(iter(step.items()))\n        print(node, str(state)[:20] + \" ...\")\n    from IPython.display import Markdown\n    def format_taxonomy_md(clusters):\n        md = \"## Final Taxonomy\\\\n\\\\n\"\n        md += \"| ID | Name | Description |\\\\n\"\n        md += \"|----|------|-------------|\\\\n\"\n        # Iterate over each inner list of dictionaries\n        for cluster_list in clusters:\n            for label in cluster_list:\n                id = label[\"id\"]\n                name = label[\"name\"].replace(\"|\", \"\\\\\\\\|\")  # Escape any pipe characters within the content\n                description = label[\"description\"].replace(\"|\", \"\\\\\\\\|\")  # Escape any pipe characters\n                md += f\"| {id} | {name} | {description} |\\\\n\"\n        return md\n    markdown_table = format_taxonomy_md(step['review_taxonomy']['clusters'])\n    display(Markdown(markdown_table))\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":38,"type":"stream"}}},"cell_type":"code","id":"1340c016-2f39-48a5-a140-12e3a402a992","outputs":[{"output_type":"stream","name":"stdout","text":"An error occurred: Connection error.\n"}],"execution_count":27}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":5}